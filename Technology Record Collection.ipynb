{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scopus Data Collection ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests \n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.optimize as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# defining all functions\n",
    "def find_nth(haystack, needle, n):  #this function is used to find strings in text   \n",
    "    start = haystack.find(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.find(needle, start+len(needle))\n",
    "        n -= 1\n",
    "    return start\n",
    "\n",
    "def countlist(x): \n",
    "    z=x\n",
    "    count_list=[]\n",
    "    while z>0:\n",
    "        n=x-(z-1)\n",
    "        count_list.append(n)\n",
    "        z=z-1\n",
    "    return count_list\n",
    "\n",
    "def update_and_calculate (tech_name):  #function that automatically calculates all the needed values for curve fitting\n",
    "    tech_name['Cumulative_Sum']=tech_name['Number_of_Publications'].cumsum() \n",
    "    tech_name['Year_initialyear']=tech_name['Year']-(tech_name['Year'].min())\n",
    "    tech_name['Normalized_Records']=tech_name['Cumulative_Sum']/(tech_name['Cumulative_Sum'].max())\n",
    "    \n",
    "\n",
    "def my_logistic(t,a,b): #logistic curve function \n",
    "    return 1/(1+a*np.exp(-b*t))\n",
    "\n",
    "\n",
    "def logistic_curve_fitting(tech_name): #function used to logistic curve fit,calculating MSE and returning S value (*100)\n",
    "    p0=np.array([0.5,0.5])\n",
    "    bounds=((0,-np.inf),(np.inf,np.inf))\n",
    "    x=tech_name.Year_initialyear\n",
    "    y=tech_name.Normalized_Records\n",
    "    (a,b),cov=optim.curve_fit(my_logistic,x,y,bounds=bounds,p0=p0) \n",
    "    MSE=mean_squared_error(y,my_logistic(x,a,b))\n",
    "    S=(np.sqrt(MSE))*100\n",
    "    S_Scopus_values.append(S)\n",
    "    return S_Scopus_values\n",
    "    \n",
    "\n",
    "\n",
    "df_0= pd.read_csv('Techs2query_scopus_updatedqueries.csv',encoding= 'unicode_escape') # reading csv containing technology names and their queries as a datafarme\n",
    "\n",
    "###the following section will send API requests and automatically extract dates+number of publications###\n",
    "\n",
    "api_url='https://api.elsevier.com/content/search/scopus'\n",
    "datelist=list(range(1976,2020)) #specify the date range of your search \n",
    "techlist=df_0['Technology'].to_list()\n",
    "querylist=df_0['Query'].to_list() \n",
    "year_list=[]\n",
    "TotalResults_list=[]\n",
    "q_list=[]\n",
    "tech_list=[]\n",
    "for q in querylist:\n",
    "    for year in datelist:\n",
    "            qu='TITLE-ABS-KEY('+q+')' #required format to search in title or abstract by Scopus\n",
    "            params={'query':(qu),'date':(year),'sort':'pubyear','content':'core','apiKey':'f5844ea038f43bc46bf65a10546c1c31'}\n",
    "            r=requests.get(api_url,params=params)\n",
    "            word = r.text\n",
    "            num_lines = word.count('opensearch:totalResults')\n",
    "            for i in range(num_lines):\n",
    "                    year_list.append(year)\n",
    "                    q_list.append(q)\n",
    "                    index=querylist.index(q)\n",
    "                    tech_list.append(techlist[index])\n",
    "                    index_start=find_nth(r.text,'opensearch:totalResults',i+1)\n",
    "                    index_end = word.find('\"', index_start+26,len(word))\n",
    "                    TotalResults=word[index_start+26:index_end]\n",
    "                    TotalResults_list.append(word[index_start+26:index_end])\n",
    "                    \n",
    "\n",
    "#Technologies, their recorded year and document count values are saved locally as a csv file\n",
    "array=np.column_stack((tech_list,q_list,year_list,TotalResults_list)) #combined document count and respective year scraped values into a single csv file \n",
    "f = open('Scopus_API_Data.csv', 'w')\n",
    "with f:\n",
    "    writer=csv.writer(f)\n",
    "    f.write(\"Technology\" + \",\"+ \"Query\"+ \",\" +\"Year\" + \",\" + \"Number_of_Publications\" + \"\\n\")\n",
    "    writer.writerows(array)\n",
    "f.close()\n",
    "\n",
    "df= pd.read_csv('Scopus_API_Data.csv',encoding= 'unicode_escape') #csv file converted to dataframe for further analysis\n",
    "\n",
    "\n",
    "\n",
    "### the following code will perform data cleaning, additional calculations required for non linear regression (curve fitting), and finally apply non linear regression and record the Standard Error of Regression (S) value####\n",
    "\n",
    "Techs = [ frame for Technology, frame in df.groupby('Technology') ]  \n",
    "Techs_no_results=[]\n",
    "Tech_name_list=[]\n",
    "S_Scopus_values=[]\n",
    "S_zero_values=[]\n",
    "\n",
    "i=0\n",
    "for i in range (len(Techs)):\n",
    "    y=0\n",
    "    for j in range (len(Techs[i])): #checking if any of the technologies showed no document count results ( all 0's)\n",
    "        x=len(Techs[i])\n",
    "        if Techs[i].iloc[j][3]==0:\n",
    "            y=y+1\n",
    "    if y==x: # if/else will only perform data cleaning and non linear regression for technologies with results/records. Technologies with zero results are excluded and saved as seperately\n",
    "        Tech_name=Techs[i].tail(1)['Technology'].values[0] \n",
    "        Techs_no_results.append(Tech_name)\n",
    "        S_zero='no scientific publications found'\n",
    "        S_zero_values.append(S_zero)\n",
    "    else:  \n",
    "        Tech_name=Techs[i].tail(1)['Technology'].values[0] \n",
    "        Tech_name_list.append(Tech_name)\n",
    "        Techs[i]= Techs[i][Techs[i].Number_of_Publications != 0]\n",
    "        Techs[i].sort_values(by=['Year'])\n",
    "        Techs[i]= Techs[i][Techs[i].Year< 2012] #this can be set according to the required range of analysis \n",
    "        csv_file_name=Tech_name+'.csv'\n",
    "        update_and_calculate(Techs[i])\n",
    "        try:\n",
    "            logistic_curve_fitting(Techs[i])\n",
    "        except:\n",
    "            S='null'\n",
    "            S_Scopus_values.append(S)\n",
    "            pass\n",
    "            \n",
    "        Techs[i].to_csv(csv_file_name) #this outputs a file containing updated csv files (to save locally as a copy/backup) of each technology containing: tech name,query,year,year-initial year, cumulative records, Normalized records\n",
    "        \n",
    "        \n",
    "        \n",
    "# A copy of technologies and their calculated S values are saved locally as a csv file.This final csv file will be used for TRL estimates.\n",
    "Tech_name_list.extend(Techs_no_results)\n",
    "S_Scopus_values.extend(S_zero_values)\n",
    "array=np.column_stack((Tech_name_list,S_Scopus_values)) \n",
    "f = open('Techs_S_Scopus_Values.csv', 'w')\n",
    "with f:\n",
    "    writer=csv.writer(f)\n",
    "    f.write(\"Technology\" + \",\"+ \"S_Scopus\" \"\\n\")\n",
    "    writer.writerows(array)\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###The following code is used to automatically plot all technologies and the logistic fits and save a .png copy locally###\n",
    "\n",
    "\n",
    "\n",
    "def my_logistic(t,a,b): #logistic curve function redefined to include k, this will allow plotting y as Cumulative Sum records instead of Normalized records\n",
    "    k=data.Cumulative_Sum.max()\n",
    "    return k/(1+a*np.exp(-b*t))\n",
    "\n",
    "for i in range (len(Techs)):\n",
    "    try:\n",
    "        Tech_name=Techs[i].tail(1)['Technology'].values[0]\n",
    "        data=Techs[i]\n",
    "        year=data.Year_initialyear\n",
    "        records=data.Cumulative_Sum\n",
    "        x=data.Year\n",
    "        p0=([0.5,0.5])\n",
    "        bounds=((0,-np.inf),(np.inf,np.inf))\n",
    "        (a,b),cov=curve_fit(my_logistic,year,records,p0=p0,bounds=bounds)\n",
    "        plt.plot(x,my_logistic(year,a,b))\n",
    "        plt.scatter(x,records)\n",
    "        plt.title(Tech_name)\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Scientific Publications')\n",
    "        plt.legend(['Logistic Model','Real data'])\n",
    "        filename=Tech_name+'.png'\n",
    "        plt.savefig(filename)\n",
    "        plt.show()\n",
    "\n",
    "    except:\n",
    "        print (Tech_name,\"error\") #some technologies that have no publications, or lack of fit will be skipped over and printed as \"error\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USPTO/PatentsView Data Collection ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests \n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.optimize as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# defining all functions\n",
    "def find_nth(haystack, needle, n):  #this function is used to find strings in text   \n",
    "    start = haystack.find(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.find(needle, start+len(needle))\n",
    "        n -= 1\n",
    "    return start\n",
    "\n",
    "def countlist(x): \n",
    "    z=x\n",
    "    count_list=[]\n",
    "    while z>0:\n",
    "        n=x-(z-1)\n",
    "        count_list.append(n)\n",
    "        z=z-1\n",
    "    return count_list\n",
    "\n",
    "def update_and_calculate (tech_name):  #function that automatically calculates all the needed values for curve fitting\n",
    "    tech_name['Cumulative_Sum']=tech_name['Number_of_Publications'].cumsum() \n",
    "    tech_name['Year_initialyear']=tech_name['Year']-(tech_name['Year'].min())\n",
    "    tech_name['Normalized_Records']=tech_name['Cumulative_Sum']/(tech_name['Cumulative_Sum'].max())\n",
    "    \n",
    "\n",
    "def my_logistic(t,a,b): #logistic curve function \n",
    "    return 1/(1+a*np.exp(-b*t))\n",
    "\n",
    "\n",
    "def logistic_curve_fitting(tech_name): #function used to logistic curve fit,calculating MSE and returning S value (*100)\n",
    "    p0=np.array([0.5,0.5])\n",
    "    bounds=((0,-np.inf),(np.inf,np.inf))\n",
    "    x=tech_name.Year_initialyear\n",
    "    y=tech_name.Normalized_Records\n",
    "    (a,b),cov=optim.curve_fit(my_logistic,x,y,bounds=bounds,p0=p0) \n",
    "    MSE=mean_squared_error(y,my_logistic(x,a,b))\n",
    "    S=(np.sqrt(MSE))*100\n",
    "    S_USPTO_values.append(S)\n",
    "    return S_USPTO_values\n",
    "    \n",
    "\n",
    "\n",
    "df_0=pd.read_csv('Techs2query_USPTO_updatedqueries.csv',encoding= 'unicode_escape') # reading csv containing technology names and their queries as a datafarme\n",
    "techlist=df_0['Technology'].to_list()\n",
    "df_0=df_0.drop(columns=['Technology'])\n",
    "\n",
    "\n",
    "\n",
    "### USPTO Automated Query Formation: the following code takes keywords in the csv file and automatically formats them to PatentsView required API request format###\n",
    "\n",
    "#counter to check for OR statements (i.e. X OR Y), AND (i.e. X AND Y) statements, or ANDOR statements (i.e. X AND (Y OR Z))\n",
    "OR=0  \n",
    "AND=0\n",
    "ANDOR=0\n",
    "for i in range (len(df_0.columns)):           \n",
    "    if (\"OR_\" in df_0.columns[i]):\n",
    "        OR=OR+1\n",
    "    if(\"AND_\" in df_0.columns[i]):\n",
    "         AND=AND+1\n",
    "    if (\"ANDOR\" in df_0.columns[i]):\n",
    "        ANDOR=ANDOR+1\n",
    "\n",
    "\n",
    "check=0\n",
    "querylist=[]\n",
    "query_text='{\"_and\":['\n",
    "query_text_or=query_text+'{\"_or\":['\n",
    "query_text_andor=''\n",
    "ANDOR_check=0\n",
    "\n",
    "#loop that will take every term and tranform it to PatentsView's required format to create a final query \n",
    "for j in range (len(df_0)):\n",
    "    #check=0\n",
    "    for i in range (len(df_0.columns)):   \n",
    "        \n",
    "        if (\"OR_\" in df_0.columns[i] and df_0.iloc[j][i]!='\"\"'):\n",
    "                \n",
    "\n",
    "                term=df_0.iloc[j][i]\n",
    "                query_text_or=query_text_or+'{\"_or\":[{\"_text_phrase\":{\"patent_abstract\":'+term+'}}'+',{\"_text_phrase\":{\"patent_title\":'+term+'}}]},'\n",
    "           \n",
    "        if ((i==OR-1) and '[{\"_or\":[{\"_or\":' in query_text_or):\n",
    "            query_text_or=query_text_or[:-1]\n",
    "            query_text_or=query_text_or+\"]},\"\n",
    "            \n",
    "        if (\"AND_\" in df_0.columns[i] and df_0.iloc[j][i]!='\"\"'):\n",
    "                #print(\"o:\",j,i)\n",
    "                if ('[{\"_or\":[{\"_or\":' in query_text_or):\n",
    "                    \n",
    "                    term=df_0.iloc[j][i]\n",
    "                    query_text_or=query_text_or+'{\"_or\":[{\"_text_phrase\":{\"patent_abstract\":'+term+'}}'+',{\"_text_phrase\":{\"patent_title\":'+term+'}}]},'\n",
    "                else:\n",
    "                    check=8\n",
    "                \n",
    "                    term=df_0.iloc[j][i]\n",
    "                    print(term)\n",
    "                    query_text=query_text+'{\"_or\":[{\"_text_phrase\":{\"patent_abstract\":'+term+'}}'+',{\"_text_phrase\":{\"patent_title\":'+term+'}}]},'\n",
    "               \n",
    "        \n",
    "        if (\"ANDOR\" in df_0.columns[i] and df_0.iloc[j][i]!='\"\"'):\n",
    "            term=df_0.iloc[j][i]\n",
    "            query_text_andor=query_text_andor+'{\"_or\":[{\"_text_phrase\":{\"patent_abstract\":'+term+'}}'+',{\"_text_phrase\":{\"patent_title\":'+term+'}}]},'\n",
    "            #print (query_text_andor)\n",
    "            ANDOR_check=1\n",
    "            \n",
    "        if ((i==(OR+AND+ANDOR)-1) and ANDOR_check==1):\n",
    "            query_text_andor='{\"_or\":['+query_text_andor\n",
    "            query_text_andor=query_text_andor[:-1]\n",
    "            query_text_andor=query_text_andor+\"]},\" \n",
    "            query_text_or=query_text_or+query_text_andor\n",
    "            query_text_andor=''\n",
    "                           \n",
    "    \n",
    "    print(check)\n",
    "    if (check==0):        \n",
    "        querylist.append(query_text_or)\n",
    "    else:\n",
    "        querylist.append(query_text)\n",
    "        \n",
    "    query_text='{\"_and\":['\n",
    "    query_text_or=query_text+'{\"_or\":['\n",
    "    check=0\n",
    "    ANDOR_check=0\n",
    "    \n",
    "\n",
    "\n",
    "###the following section will send API requests and automatically extract dates+number of publications###\n",
    "\n",
    "  \n",
    "TotalResults_list=[] \n",
    "tech_list=[]\n",
    "q_list=[]\n",
    "date_list=list(range(1976,2020)) #enter the search range here \n",
    "date_list_string=[str(s) for s in date_list]\n",
    "year_list=[]\n",
    "for q in querylist:\n",
    "    for year in date_list_string:\n",
    "        r=requests.get('https://www.patentsview.org/api/patents/query?q='+q+'{\"_gte\":{\"app_date\":\"'+year+'-01-01\"}},{\"_lte\":{\"app_date\":\"'+year+'-12-31\"}}]}&f=[\"app_date\"]')\n",
    "        word = r.text\n",
    "        num_lines = word.count('total_patent_count')  \n",
    "        for i in range(num_lines):\n",
    "            q_list.append(q)\n",
    "            index=querylist.index(q)\n",
    "            tech_list.append(techlist[index])\n",
    "            year_list.append(year)\n",
    "            index_start=find_nth(r.text,'total_patent_count',i+1)\n",
    "            index_end= word.find('\"', index_start+20,len(word))\n",
    "            TotalResults_list.append(word[index_start+20:index_end])\n",
    "        \n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "#Technologies, their recorded year and document count values are saved locally as a csv file\n",
    "array=np.column_stack((tech_list,q_list,year_list,TotalResults_list)) #combined document count and respective year scraped values into a single csv file \n",
    "f = open('USPTO_API_Data.csv', 'w')\n",
    "with f:\n",
    "    writer=csv.writer(f)\n",
    "    f.write(\"Technology\" + \",\"+ \"Query\"+ \",\" +\"Year\" + \",\" + \"Number_of_Publications\" + \"\\n\")\n",
    "    writer.writerows(array)\n",
    "f.close()\n",
    "\n",
    "df= pd.read_csv('USPTO_API_Data.csv',encoding= 'unicode_escape') #csv file converted to dataframe for further analysis\n",
    "\n",
    "\n",
    "\n",
    "### the following code will perform data cleaning, additional calculations required for non linear regression (curve fitting), and finally apply non linear regression and record the Standard Error of Regression (S) value####\n",
    "\n",
    "Techs = [ frame for Technology, frame in df.groupby('Technology') ]  \n",
    "Techs_no_results=[]\n",
    "Tech_name_list=[]\n",
    "S_USPTO_values=[]\n",
    "S_zero_values=[]\n",
    "i=0\n",
    "for i in range (len(Techs)):\n",
    "    y=0\n",
    "    for j in range (len(Techs[i])): #checking if any of the technologies showed no document count results ( all 0's)\n",
    "        x=len(Techs[i])\n",
    "        if Techs[i].iloc[j][3]==0:\n",
    "            y=y+1\n",
    "    if y==x: # if/else will only perform data cleaning and non linear regression for technologies with results/records. Technologies with zero results are excluded and saved as seperately\n",
    "        Tech_name=Techs[i].tail(1)['Technology'].values[0] \n",
    "        Techs_no_results.append(Tech_name)\n",
    "        S_zero='no patents found'\n",
    "        S_zero_values.append(S_zero)\n",
    "    else:  \n",
    "        Tech_name=Techs[i].tail(1)['Technology'].values[0] \n",
    "        Tech_name_list.append(Tech_name)\n",
    "        Techs[i]= Techs[i][Techs[i].Number_of_Publications != 0]\n",
    "        Techs[i].sort_values(by=['Year'])\n",
    "        Techs[i]= Techs[i][Techs[i].Year< 2012] #this can be set according to the required range of analysis \n",
    "        csv_file_name=Tech_name+'.csv'\n",
    "        update_and_calculate(Techs[i])\n",
    "        try:\n",
    "            logistic_curve_fitting(Techs[i])\n",
    "        except:\n",
    "            S='null'\n",
    "            S_USPTO_values.append(S)\n",
    "            pass\n",
    "            \n",
    "        Techs[i].to_csv(csv_file_name) #this outputs a file containing updated csv files (to save locally as a copy/backup) of each technology containing: tech name,query,year,year-initial year, cumulative records, Normalized records\n",
    "        \n",
    "        \n",
    "        \n",
    "# A copy of technologies and their calculated S values are saved locally as a csv file.This final csv file will be used for TRL estimates.\n",
    "Tech_name_list.extend(Techs_no_results)\n",
    "S_USPTO_values.extend(S_zero_values)\n",
    "array=np.column_stack((Tech_name_list,S_USPTO_values)) \n",
    "f = open('Techs_S_USPTO_Values.csv', 'w')\n",
    "with f:\n",
    "    writer=csv.writer(f)\n",
    "    f.write(\"Technology\" + \",\"+ \"S_USPTO\" \"\\n\")\n",
    "    writer.writerows(array)\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "###The following code is used to automatically plot all technologies and the logistic fits and save a .png copy locally###\n",
    "\n",
    "\n",
    "\n",
    "def my_logistic(t,a,b): #logistic curve function redefined to include k, this will allow plotting y as Cumulative Sum records instead of Normalized records\n",
    "    k=data.Cumulative_Sum.max()\n",
    "    return k/(1+a*np.exp(-b*t))\n",
    "\n",
    "for i in range (len(Techs)):\n",
    "    try:\n",
    "        Tech_name=Techs[i].tail(1)['Technology'].values[0]\n",
    "        data=Techs[i]\n",
    "        year=data.Year_initialyear\n",
    "        records=data.Cumulative_Sum\n",
    "        x=data.Year\n",
    "        p0=([0.5,0.5])\n",
    "        bounds=((0,-np.inf),(np.inf,np.inf))\n",
    "        (a,b),cov=curve_fit(my_logistic,year,records,p0=p0,bounds=bounds)\n",
    "        plt.plot(x,my_logistic(year,a,b))\n",
    "        plt.scatter(x,records)\n",
    "        plt.title(Tech_name)\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Patents')\n",
    "        plt.legend(['Logistic Model','Real data'])\n",
    "        filename=Tech_name+'.png'\n",
    "        plt.savefig(filename)\n",
    "        plt.show()\n",
    "\n",
    "    except:\n",
    "        print (Tech_name,\"error\") #some technologies that have no publications, or lack of fit will be skipped over and printed as \"error\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factiva Data Collection ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests \n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.optimize as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# defining all functions\n",
    "def find_nth(haystack, needle, n):  #this function is used to find strings in text   \n",
    "    start = haystack.find(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.find(needle, start+len(needle))\n",
    "        n -= 1\n",
    "    return start\n",
    "\n",
    "def countlist(x): \n",
    "    z=x\n",
    "    count_list=[]\n",
    "    while z>0:\n",
    "        n=x-(z-1)\n",
    "        count_list.append(n)\n",
    "        z=z-1\n",
    "    return count_list\n",
    "\n",
    "\n",
    "def find_in_list_of_list(mylist, char): \n",
    "    for sub_list in mylist:\n",
    "        if char in sub_list:\n",
    "            x=mylist.index(sub_list)\n",
    "            y=sub_list.index(char)\n",
    "            return (x,y)\n",
    "            \n",
    "    raise ValueError(\"'{char}' is not in list\".format(char = char))\n",
    "    \n",
    "\n",
    "def returnNotMatches(a, b): #function will be used to flag queries that don't match ( will return non matches)\n",
    "    return [[x for x in a if x not in b], [x for x in b if x not in a]]\n",
    "\n",
    "def update_and_calculate (tech_name):  #function that automatically calculates all the needed values for curve fitting\n",
    "    tech_name['Cumulative_Sum']=tech_name['Number_of_Publications'].cumsum() \n",
    "    tech_name['Year_initialyear']=tech_name['Year']-(tech_name['Year'].min())\n",
    "    tech_name['Normalized_Records']=tech_name['Cumulative_Sum']/(tech_name['Cumulative_Sum'].max())\n",
    "    \n",
    "\n",
    "def my_logistic(t,a,b): #logistic curve function \n",
    "    return 1/(1+a*np.exp(-b*t))\n",
    "\n",
    "\n",
    "def logistic_curve_fitting(tech_name): #function used to logistic curve fit,calculating MSE and returning S value (*100)\n",
    "    p0=np.array([0.5,0.5])\n",
    "    bounds=((0,-np.inf),(np.inf,np.inf))\n",
    "    x=tech_name.Year_initialyear\n",
    "    y=tech_name.Normalized_Records\n",
    "    (a,b),cov=optim.curve_fit(my_logistic,x,y,bounds=bounds,p0=p0) \n",
    "    MSE=mean_squared_error(y,my_logistic(x,a,b))\n",
    "    S=(np.sqrt(MSE))*100\n",
    "    S_Factiva_values.append(S)\n",
    "    return S_Factiva_values\n",
    "    \n",
    "df_0= pd.read_csv('Techs2query_Factiva_updatedqueries.csv',encoding= 'unicode_escape')  # reading csv containing technology names and their queries as a datafarme\n",
    "techlist=df_0['Technology'].to_list()\n",
    "querylist=df_0['Query'].to_list()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### The following code will automatically read every csv file of each technology, tranform into a dictionary, and extract document counts, year, and query.\n",
    "\n",
    "filenames = glob(r'C:\\Users\\Safa\\A_Research_UofT\\Data Collection\\Data Collection_Factiva\\Tech_CSV_files\\ChartSummary*.csv')\n",
    "Year_list_cleaned=[]\n",
    "Tech_query_list=[]\n",
    "Tech_query_short=[]\n",
    "Documentcount_list=[]\n",
    "Technology_list=[]\n",
    "for f in filenames:\n",
    "    with open(f, 'r') as read_obj:\n",
    "        dictio=[] \n",
    "        # pass the file object to reader() to get the reader object\n",
    "        csv_reader = reader(read_obj)\n",
    "    # Iterate over each row in the csv using reader object\n",
    "        for row in csv_reader: # row variable is a list that represents a row in csv\n",
    "            dictio.append(row)\n",
    "\n",
    "    \n",
    "    Year_list=[]\n",
    "    (a,b)=find_in_list_of_list(dictio, 'Date') #locating index of each term \n",
    "    (c,d)=find_in_list_of_list(dictio, 'Search Summary')\n",
    "    (e,f)=find_in_list_of_list(dictio,'Text ')\n",
    "    Tech_query=dictio[e][f+1]\n",
    "\n",
    "    \n",
    "    i=5\n",
    "   \n",
    "        \n",
    "    for row in dictio:\n",
    "        while (a-2)<i<(c-2):\n",
    "            Year_list.append(dictio[i][0])\n",
    "            Documentcount_list.append(dictio[i][1])\n",
    "            i=i+1\n",
    "    i=0\n",
    "    for row in Year_list:\n",
    "        index_start_t=find_nth(row,'Start Date',i+1)\n",
    "        index_end_t = row.find(' End Date', index_start_t+22,len(row))\n",
    "        TimeBin=row[index_start_t+22:index_end_t]\n",
    "        Year_list_cleaned.append(row[index_start_t+22:index_end_t])\n",
    "        i=i+1\n",
    "        Tech_query_list.append(Tech_query)\n",
    "        \n",
    "        index=df_0[df_0['Query']==Tech_query].index.values #locating the index of the technology name by its query \n",
    "        index.astype(int)\n",
    "        x=index[0]\n",
    "        Techname=df_0.iloc[x][0]\n",
    "        Technology_list.append(Techname)\n",
    "\n",
    "    \n",
    "    \n",
    "#part of code that flags mistakes in queries\n",
    "returnNotMatches(Tech_query_list, querylist)\n",
    "\n",
    "\n",
    "#Technologies, their recorded year and document count values are saved locally as a csv file\n",
    "array=np.column_stack((Technology_list,Tech_query_list,Year_list_cleaned,Documentcount_list)) #combined document count and respective year scraped values into a single csv file and outputs it so that a physical copy is available prior to further analysis\n",
    "f = open('Factiva_API_Data.csv', 'w')\n",
    "with f:\n",
    "    writer=csv.writer(f)\n",
    "    f.write(\"Technology\" + \",\" +\"Query\"+ \",\"+ \"Year\" + \",\" + \"Number_of_Publications\" + \"\\n\")\n",
    "    writer.writerows(array)\n",
    "f.close()\n",
    "df= pd.read_csv('Factiva_API_Data.csv',encoding= 'unicode_escape')  #csv file converted to dataframe for further analysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### the following code will perform data cleaning, additional calculations required for non linear regression (curve fitting), and finally apply non linear regression and record the Standard Error of Regression (S) value####\n",
    "\n",
    "Techs = [ frame for Technology, frame in df.groupby('Technology') ]  \n",
    "Techs_no_results=[]\n",
    "Tech_name_list=[]\n",
    "S_Factiva_values=[]\n",
    "S_zero_values=[]\n",
    "i=0\n",
    "for i in range (len(Techs)):\n",
    "    y=0\n",
    "    for j in range (len(Techs[i])): #checking if any of the technologies showed no document count results ( all 0's)\n",
    "        x=len(Techs[i])\n",
    "        if Techs[i].iloc[j][3]==0:\n",
    "            y=y+1\n",
    "    if y==x: # if/else will only perform data cleaning and non linear regression for technologies with results/records. Technologies with zero results are excluded and saved as seperately\n",
    "        Tech_name=Techs[i].tail(1)['Technology'].values[0] \n",
    "        Techs_no_results.append(Tech_name)\n",
    "        S_zero='no patents found'\n",
    "        S_zero_values.append(S_zero)\n",
    "    else:  \n",
    "        Tech_name=Techs[i].tail(1)['Technology'].values[0] \n",
    "        Tech_name_list.append(Tech_name)\n",
    "        Techs[i]= Techs[i][Techs[i].Number_of_Publications != 0]\n",
    "        Techs[i].sort_values(by=['Year'])\n",
    "        Techs[i]= Techs[i][Techs[i].Year< 2012] #this can be set according to the required range of analysis \n",
    "        csv_file_name=Tech_name+'.csv'\n",
    "        update_and_calculate(Techs[i])\n",
    "        try:\n",
    "            logistic_curve_fitting(Techs[i])\n",
    "        except:\n",
    "            S='null'\n",
    "            S_Factiva_values.append(S)\n",
    "            pass\n",
    "            \n",
    "        Techs[i].to_csv(csv_file_name) #this outputs a file containing updated csv files (to save locally as a copy/backup) of each technology containing: tech name,query,year,year-initial year, cumulative records, Normalized records\n",
    "        \n",
    "        \n",
    "        \n",
    "# A copy of technologies and their calculated S values are saved locally as a csv file.This final csv file will be used for TRL estimates.\n",
    "Tech_name_list.extend(Techs_no_results)\n",
    "S_Factiva_values.extend(S_zero_values)\n",
    "array=np.column_stack((Tech_name_list,S_Factiva_values)) \n",
    "f = open('Techs_S_Factiva_Values.csv', 'w')\n",
    "with f:\n",
    "    writer=csv.writer(f)\n",
    "    f.write(\"Technology\" + \",\"+ \"S_Factiva\" \"\\n\")\n",
    "    writer.writerows(array)\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "###The following code is used to automatically plot all technologies and the logistic fits and save a .png copy locally###\n",
    "\n",
    "\n",
    "\n",
    "def my_logistic(t,a,b): #logistic curve function redefined to include k, this will allow plotting y as Cumulative Sum records instead of Normalized records\n",
    "    k=data.Cumulative_Sum.max()\n",
    "    return k/(1+a*np.exp(-b*t))\n",
    "\n",
    "for i in range (len(Techs)):\n",
    "    try:\n",
    "        Tech_name=Techs[i].tail(1)['Technology'].values[0]\n",
    "        data=Techs[i]\n",
    "        year=data.Year_initialyear\n",
    "        records=data.Cumulative_Sum\n",
    "        x=data.Year\n",
    "        p0=([0.5,0.5])\n",
    "        bounds=((0,-np.inf),(np.inf,np.inf))\n",
    "        (a,b),cov=curve_fit(my_logistic,year,records,p0=p0,bounds=bounds)\n",
    "        plt.plot(x,my_logistic(year,a,b))\n",
    "        plt.scatter(x,records)\n",
    "        plt.title(Tech_name)\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('News Records')\n",
    "        plt.legend(['Logistic Model','Real data'])\n",
    "        filename=Tech_name+'.png'\n",
    "        plt.savefig(filename)\n",
    "        plt.show()\n",
    "\n",
    "    except:\n",
    "        print (Tech_name,\"error\") #some technologies that have no publications, or lack of fit will be skipped over and printed as \"error\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
